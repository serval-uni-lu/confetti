import pandas as pd
import numpy as np
from numpy.f2py.crackfortran import param_parse

from confetti.explainer.utils import load_data, convert_string_to_array, load_multivariate_ts_from_csv
import keras
import config as cfg
import tensorflow as tf
tf.keras.utils.disable_interactive_logging()


class Evaluator:
    """
    Evaluates the quality of counterfactual explanations generated by an explainer.
    Computes the metrics coverage, sparsity, confidence, validity, and proximity.
    """

    def __init__(self):
        """
        Initializes the Evaluator object
        """

    def evaluate_from_csv(self, explainer:str, dataset: str, model_name:str, alpha: bool=True, param_config: float=0.0):
        """
        Evaluates the counterfactual explanations for a given dataset.

        Args:
            explainer (str): The name of the explainer.
            dataset (str): The name of the dataset to evaluate.
            model_name (str): The name of the model that was used. Either 'fcn' or 'resnet'.
            alpha (bool): Only applies when explainer = 'confetti'. If True, loads alpha solutions, otherwise loads
            theta solutions.
            param_config (float): Only applies when explainer = 'confetti'. The parameter configuration used for the
            explainer.

        Returns:
            tuple: A tuple containing:
                - DataFrame with individual counterfactual metrics.
                - DataFrame summarizing the overall evaluation results.
        """
        # Load Data
        X_sample, y_sample = load_multivariate_ts_from_csv(f'../data/{dataset}_samples.csv')
        timesteps = X_sample.shape[1]  # Number of time steps per instance
        channels = X_sample.shape[2]  # Number of channels per instance

        # Load Model
        model = self.__get_model(dataset, model_name)

        # Get Counterfactuals
        counterfactuals = self.__get_counterfactuals(explainer, dataset, model_name, alpha, param_config)
        counterfactuals['Solution'] = counterfactuals['Solution'].apply(lambda x: convert_string_to_array(x,
                                                                                                          timesteps=timesteps,
                                                                                                          channels=channels))

        # Compute Metrics
        counterfactuals_metrics = self.__compute_metrics(model=model,
                                                         counterfactuals=counterfactuals,
                                                         sample=X_sample,
                                                         og_labels=y_sample,
                                                         timesteps=timesteps,
                                                         channels=channels)

        dataset_summary = self.__get_dataset_summary(explainer=explainer,
                                                    dataset=dataset,
                                                     sample=X_sample,
                                                     counterfactuals=counterfactuals,
                                                     metrics=counterfactuals_metrics,
                                                     alpha=alpha,
                                                     param_config=param_config)

        return counterfactuals_metrics, dataset_summary

    def evaluate_results(self, model, explainer:str, dataset: str, counterfactuals:pd.DataFrame, sample: np.array, og_labels: np.array,
                         timesteps: int, channels: int, alpha: bool = True, param_config: float = 0.0):
        """
        Evaluates the counterfactuals using the provided test data.

        Args:
            model: The model to evaluate.
            explainer (str): The name of the explainer.
            dataset (str): The name of the dataset.
            counterfactuals (DataFrame): The counterfactuals to evaluate.
            sample (np.array): The sample set containing original instances.
            og_labels (np.array): The original labels of the instances.
            timesteps (int): Number of time steps per instance.
            channels (int): Number of channels per instance.

        Returns:
            DataFrame: A DataFrame containing the evaluation results.
        """

        # Compute Metrics
        counterfactuals_metrics = self.__compute_metrics(model=model,
                                                         counterfactuals=counterfactuals,
                                                         sample=sample,
                                                         og_labels=og_labels,
                                                         timesteps=timesteps,
                                                         channels=channels)
        dataset_summary = self.__get_dataset_summary(dataset=dataset,
                                                        explainer=explainer,
                                                        sample=sample,
                                                        counterfactuals=counterfactuals,
                                                        metrics=counterfactuals_metrics,
                                                        alpha=alpha,
                                                        param_config=param_config)

        return counterfactuals_metrics, dataset_summary

    def __load_samples(self, dataset: str):
        """
        Loads the sample set for a given dataset.

        Args:
            dataset (str): The name of the dataset.

        Returns:
            tuple: A tuple containing the sample set and labels.
        """
        sample_file = f"{cfg.DATA_DIR}/{dataset}_samples.csv"
        X_samples, y_samples = load_multivariate_ts_from_csv(sample_file)

        return X_samples, y_samples

    def __compute_metrics(self, model, counterfactuals: pd.DataFrame, sample: np.array, og_labels: np.array, timesteps,
                          channels):
        """
        Computes the evaluation metrics for a given counterfactual instance.

        Args:
            model (keras.Model): The trained model for making predictions.
            counterfactuals (DataFrame): The counterfactuals to evaluate.
            sample (numpy.ndarray): The sample set containing the original instances.
            timesteps (int): Number of time steps per instance.
            channels (int): Number of channels per instance.

        Returns:
            DataFrame: A DataFrame containing the evaluation results.
        """
        metrics = pd.DataFrame({
            'Sparsity': pd.Series(dtype='float'),
            'Confidence': pd.Series(dtype='float'),
            'Validity': pd.Series(dtype='int'),
            'Proximity L1': pd.Series(dtype='float'),
            'Proximity L2': pd.Series(dtype='float'),
            'Proximity MAD': pd.Series(dtype='float')
        })

        for i in range(len(sample)):
            instance = sample[i]
            original_label = og_labels[i]

            #Check if there is a counterfactual for this instance
            if i in counterfactuals['Test Instance'].values:
                # Get all counterfactuals for this instance
                ces = counterfactuals[counterfactuals['Test Instance'] == i]['Solution']
                for ce in ces:
                    # Compute Sparsity (how many unchanged features)
                    sparsity = self.__get_sparsity(instance, ce)

                    # Compute Confidence
                    confidence = self.__get_confidence(model, ce, timesteps, channels, original_label)

                    #Compute Validity
                    validity = self.__get_validity(model, ce, timesteps, channels, original_label)

                    # Compute Proximity (L1 MAD to original instance)
                    proximity_MAD = self.__l1_distance_mad(instance, ce)
                    #Compute Proximity (L1 distance to original instance)
                    proximity_l1 = self.__l1_distance(instance, ce)
                    # Compute Proximity (L2 distance to original instance)
                    proximity_l2 = self.__l2_distance(instance, ce)

                    # Store results
                    row_results_dict = {'Sparsity': sparsity, 'Confidence': confidence, 'Validity': validity,
                                        'Proximity L1': proximity_l1, 'Proximity L2': proximity_l2, 'Proximity MAD': proximity_MAD}
                    row_results_df = pd.DataFrame([row_results_dict])
                    metrics = pd.concat([metrics, row_results_df], ignore_index=True)
            else:
                # If no counterfactual is found, skip instance
                print(f"No counterfactual found for instance {i} in dataset. Skipping...")
                continue

        return metrics

    def __get_model(self, dataset: str, model:str):
        """
        Loads the trained model associated with the dataset.

        Args:
            dataset (str): The dataset name.
            model (str): The name of the model that was used. Either 'fcn' or 'resnet'.

        Returns:
            keras.Model: The trained Keras model.
        """
        return keras.models.load_model(str(cfg.TRAINED_MODELS_DIR / dataset / f'{dataset}_{model}.keras'))

    def __get_counterfactuals(self, explainer:str, dataset: str, model:str, alpha: bool, param_config: float):
        """
        Loads the counterfactual solutions generated by the explainer.

        Args:
            explainer (str): The name of the explainer.
            dataset (str): The dataset name.
            model (str): The name of the model that was used. Either 'fcn' or 'resnet'.
            alpha (bool): Only applies when explainer = 'confetti'. If True, loads alpha solutions, otherwise loads theta solutions.
            param_config (float): Only applies when explainer = 'confetti'. The parameter configuration used for the explainer.

        Returns:
            DataFrame: A DataFrame containing the counterfactuals.
        """
        if 'confetti' in explainer:
            if alpha:
                return pd.read_csv(cfg.RESULTS_DIR / dataset / f'{explainer}_{dataset}_{model}_alpha_{param_config}.csv')
            else:
                return pd.read_csv(cfg.RESULTS_DIR / dataset / f'{explainer}_{dataset}_{model}_theta_{param_config}.csv')
        else:
            return pd.read_csv(cfg.RESULTS_DIR / dataset / f'{explainer}_{dataset}_{model}_counterfactuals.csv')

    def __get_dataset_summary(self, explainer:str, dataset: str, sample: np.array, counterfactuals:pd.DataFrame,
                              metrics: pd.DataFrame, alpha: bool = True, param_config: float = 0.0):
        """
        Summarizes the evaluation metrics for a dataset by computing the mean values.

        Args:
            param explainer (str): The name of the explainer.
            param: dataset (str): The dataset name.
            param: sample (numpy.ndarray): The sample set containing the original instances.
            param: counterfactuals (pd.DataFrame): The counterfactuals to evaluate.
            param: metrics (DataFrame): The computed metrics for each counterfactual.
            param: alpha (bool): Only applies when explainer = 'confetti'. Only used to inform what was the parameter
            used to generate the counterfactuals.
            param: param_config (float): Only applies when explainer = 'confetti'. The parameter configuration used for
            the explainer.

        Returns:
            DataFrame: A DataFrame summarizing the evaluation results.
        """
        coverage = self.__get_coverage(sample=sample, counterfactuals=counterfactuals)
        if explainer not in ['confetti_naive', 'confetti_optimized']:
            summary = {
                'Explainer': explainer,
                'Dataset': dataset,
                'Coverage': coverage,
                'Sparsity': metrics['Sparsity'].mean(),
                'Confidence': metrics['Confidence'].mean(),
                'Validity': metrics['Validity'].mean(),
                'Proximity L1': metrics['Proximity L1'].mean(),
                'Proximity L2': metrics['Proximity L2'].mean(),
                'Proximity MAD': metrics['Proximity MAD'].mean()
            }
        else:
            summary = {
                'Explainer': explainer,
                'Alpha': alpha,
                'Param Config': param_config,
                'Dataset': dataset,
                'Coverage': coverage,
                'Sparsity': metrics['Sparsity'].mean(),
                'Confidence': metrics['Confidence'].mean(),
                'Validity': metrics['Validity'].mean(),
                'Proximity L1': metrics['Proximity L1'].mean(),
                'Proximity L2': metrics['Proximity L2'].mean(),
                'Proximity MAD': metrics['Proximity MAD'].mean()
            }

        return pd.DataFrame([summary])

    def __get_coverage(self, sample: np.array, counterfactuals: pd.DataFrame) -> float:
        """
        Compute the coverage of the counterfactuals.

        Args:
        param: sample (numpy.ndarray): The sample set containing the original instances.
        param: counterfactuals (pd.DataFrame): The counterfactuals to evaluate.

        Returns:
        float: Coverage value.
        """

        n_total = sample.shape[0]
        covered_instances = counterfactuals["Test Instance"].unique()
        n_covered = len(covered_instances)
        coverage = (n_covered / n_total) * 100
        return coverage

    def __get_sparsity(self, original: pd.DataFrame, counterfactual: pd.DataFrame):
        return np.mean(original.flatten() == counterfactual.flatten())

    def __get_confidence(self, model, counterfactual, timesteps, channels, original_label):
        """
        Compute the confidence of the counterfactual.

        Parameters:
        model (keras.Model): The trained model for making predictions.
        counterfactual (pd.DataFrame): The counterfactual instance.
        timesteps (int): Number of time steps per instance.
        channels (int): Number of channels per instance.
        original_label (int): The original label of the instance.

        Returns:
        float: Confidence value.
        """
        return 1 - model.predict(counterfactual.reshape(1, timesteps, channels))[0][original_label]

    def __get_validity(self, model, counterfactual, timesteps, channels, original_label):
        ce_label = np.argmax(model.predict(counterfactual.reshape(1, timesteps, channels)))
        return 0 if original_label == ce_label else 1

    def __l1_distance(self, ts1:np.array, ts2:np.array):
        """
        Compute the Manhattan (L1) distance between two multivariate time series.

        Parameters:
        ts1, ts2 (2D numpy arrays): Time series of shape (T, D)

        Returns:
        float: Manhattan distance.
        """
        if ts1.shape != ts2.shape:
            raise ValueError("Time series must have the same shape (Timesteps, Dimensions).")
        return np.sum(np.abs(ts1 - ts2))

    def __l2_distance(self, ts1:np.array, ts2:np.array):
        """
        Compute the Euclidean (L2) distance between two multivariate time series.

        Parameters:
        ts1, ts2 (2D numpy arrays): Time series of shape (T, D)

        Returns:
        float: Euclidean distance.
        """
        if ts1.shape != ts2.shape:
            raise ValueError("Time series must have the same shape (Timesteps, Dimensions).")
        return np.sqrt(np.sum((ts1 - ts2) ** 2))

    def __mad_normalization(self, ts):
        """
        Compute the Median Absolute Deviation (MAD) for each feature in a multivariate time series.

        Parameters:
        ts (2D numpy array): Time series of shape (T, D)

        Returns:
        1D numpy array: MAD values for each feature (D,)
        """
        median = np.median(ts, axis=0)
        mad = np.median(np.abs(ts - median), axis=0)
        mad[mad == 0] = 1e-9  # Avoid division by zero
        return mad

    def __l1_distance_mad(self, ts1:np.array, ts2:np.array):
        """
        Compute the Manhattan (L1) distance between two multivariate time series,
        normalizing the differences using MAD.

        Parameters:
        ts1, ts2 (2D numpy arrays): Time series of shape (T, D)

        Returns:
        float: Normalized Manhattan distance.
        """
        if ts1.shape != ts2.shape:
            raise ValueError("Time series must have the same shape (T, D).")

        stack = np.vstack((ts1, ts2))
        mad_values = self.__mad_normalization(stack)  # Compute MAD over both series
        return np.sum(np.abs(ts1 - ts2) / mad_values)
