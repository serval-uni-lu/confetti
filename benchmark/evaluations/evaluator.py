import pandas as pd
import numpy as np
from confetti.explainer.utils import load_data, convert_string_to_array
import keras
import config as cfg
import tensorflow as tf
tf.keras.utils.disable_interactive_logging()


class Evaluator:
    """
    Evaluates the quality of counterfactual explanations generated by an explainer.
    Computes the metrics sparsity, confidence, validity, and proximity.
    """

    def __init__(self):
        """
        Initializes the Evaluator with a specific explainer name.
        """

    def evaluate_dataset(self, explainer:str, dataset: str, model_name:str, alpha: bool=True, param_config: float=0.0):
        """
        Evaluates the counterfactual explanations for a given dataset.

        Args:
            explainer (str): The name of the explainer.
            dataset (str): The name of the dataset to evaluate.
            model_name (str): The name of the model that was used. Either 'fcn' or 'resnet'.
            alpha (bool): Only applies when explainer = 'confetti'. If True, loads alpha solutions, otherwise loads theta solutions.
            param_config (float): Only applies when explainer = 'confetti'. The parameter configuration used for the explainer.

        Returns:
            tuple: A tuple containing:
                - DataFrame with individual counterfactual metrics.
                - DataFrame summarizing the overall evaluation results.
        """
        # Load Data
        X_train, X_test, y_train, y_test = load_data(dataset, one_hot=True)
        timesteps = X_train.shape[1]  # Number of time steps per instance
        channels = X_train.shape[2]  # Number of channels per instance

        # Load Model
        model = self.__get_model(dataset, model_name)

        # Get Counterfactuals
        counterfactuals = self.__get_counterfactuals(explainer, dataset, model_name, alpha, param_config)

        # Compute Metrics
        counterfactuals_metrics = self.__get_counterfactual_metrics(counterfactuals, model, X_test, timesteps, channels)
        dataset_summary = self.__get_dataset_summary(dataset, counterfactuals_metrics)

        return counterfactuals_metrics, dataset_summary

    def __get_model(self, dataset: str, model:str):
        """
        Loads the trained model associated with the dataset.

        Args:
            dataset (str): The dataset name.
            model (str): The name of the model that was used. Either 'fcn' or 'resnet'.

        Returns:
            keras.Model: The trained Keras model.
        """
        return keras.models.load_model(str(cfg.TRAINED_MODELS_DIR / dataset / f'{dataset}_{model}.keras'))

    def __get_counterfactuals(self, explainer:str, dataset: str, model:str, alpha: bool, param_config: float):
        """
        Loads the counterfactual solutions generated by the explainer.

        Args:
            explainer (str): The name of the explainer.
            dataset (str): The dataset name.
            model (str): The name of the model that was used. Either 'fcn' or 'resnet'.
            alpha (bool): Only applies when explainer = 'confetti'. If True, loads alpha solutions, otherwise loads theta solutions.
            param_config (float): Only applies when explainer = 'confetti'. The parameter configuration used for the explainer.

        Returns:
            DataFrame: A DataFrame containing the counterfactuals.
        """
        if 'confetti' in explainer:
            if alpha:
                return pd.read_csv(cfg.RESULTS_DIR / dataset / f'{explainer}_{dataset}_{model}_alpha_{param_config}.csv')
            else:
                return pd.read_csv(cfg.RESULTS_DIR / dataset / f'{explainer}_{dataset}_{model}_theta_{param_config}.csv')
        else:
            return pd.read_csv(cfg.RESULTS_DIR / dataset / f'{explainer}_{dataset}_{model}_counterfactuals.csv')

    def __get_counterfactual_metrics(self, counterfactuals, model, X_test, timesteps, channels):
        """
        Computes key evaluation metrics for each counterfactual.

        Args:
            counterfactuals (DataFrame): The counterfactuals to evaluate.
            model (keras.Model): The trained model for making predictions.
            X_test (numpy.ndarray): The test set containing original instances.
            timesteps (int): Number of time steps per instance.
            channels (int): Number of channels per instance.

        Returns:
            DataFrame: A DataFrame containing sparsity, confidence, validity, and proximity metrics.
        """
        metrics = pd.DataFrame({
            'Sparsity': pd.Series(dtype='float'),
            'Confidence': pd.Series(dtype='float'),
            'Validity': pd.Series(dtype='int'),
            'Proximity': pd.Series(dtype='float')
        })

        for i in range(len(counterfactuals)):
            instance = counterfactuals.iloc[i]['Test Instance']
            original_label = np.argmax(model.predict(X_test[instance].reshape(1, timesteps, channels)))
            counterfactual = convert_string_to_array(counterfactuals.iloc[instance]['Solution'],
                                                     timesteps=timesteps, channels=channels)

            # Compute Sparsity (how many unchanged features)
            sparsity = self.__get_sparsity(X_test[instance], counterfactual)

            # Compute Confidence (how different the prediction probability is from the original class)
            confidence = self.__get_confidence(model, counterfactual, timesteps, channels, original_label)

            # Compute Validity (whether the counterfactual leads to a class change)
            validity = self.__get_validity(model, counterfactual, timesteps, channels, original_label)

            # Compute Proximity (L1 distance to original instance)
            proximity = self.__l1_distance_mad(X_test[instance], counterfactual)

            # Store results
            row_results_dict = {'Sparsity': sparsity, 'Confidence': confidence, 'Validity': validity,
                                'Proximity': proximity}
            row_results_df = pd.DataFrame([row_results_dict])
            metrics = pd.concat([metrics, row_results_df], ignore_index=True)

        return metrics

    def __get_dataset_summary(self, dataset: str, metrics: pd.DataFrame):
        """
        Summarizes the evaluation metrics for a dataset by computing the mean values.

        Args:
            dataset (str): The dataset name.
            metrics (DataFrame): The computed metrics for each counterfactual.

        Returns:
            DataFrame: A DataFrame summarizing the evaluation results.
        """
        summary = {
            'Dataset': dataset,
            'Sparsity': metrics['Sparsity'].mean(),
            'Confidence': metrics['Confidence'].mean(),
            'Validity': metrics['Validity'].mean(),
            'Proximity': metrics['Proximity'].mean()
        }
        return pd.DataFrame([summary])

    def __get_sparsity(self, original: pd.DataFrame, counterfactual: pd.DataFrame):
        return np.mean(original.flatten() == counterfactual.flatten())

    def __get_confidence(self, model, counterfactual, timesteps, channels, original_label):
        """
        Compute the confidence of the counterfactual.

        Parameters:
        model (keras.Model): The trained model for making predictions.
        counterfactual (pd.DataFrame): The counterfactual instance.
        timesteps (int): Number of time steps per instance.
        channels (int): Number of channels per instance.
        original_label (int): The original label of the instance.

        Returns:
        float: Confidence value.
        """
        return 1 - model.predict(counterfactual.reshape(1, timesteps, channels))[0][original_label]

    def __get_validity(self, model, counterfactual, timesteps, channels, original_label):
        ce_label = np.argmax(model.predict(counterfactual.reshape(1, timesteps, channels)))
        return 0 if original_label == ce_label else 1

    def __mad_normalization(self, ts):
        """
        Compute the Median Absolute Deviation (MAD) for each feature in a multivariate time series.

        Parameters:
        ts (2D numpy array): Time series of shape (T, D)

        Returns:
        1D numpy array: MAD values for each feature (D,)
        """
        median = np.median(ts, axis=0)
        mad = np.median(np.abs(ts - median), axis=0)
        mad[mad == 0] = 1e-9  # Avoid division by zero
        return mad

    def __l1_distance_mad(self, ts1:np.array, ts2:np.array):
        """
        Compute the Manhattan (L1) distance between two multivariate time series,
        normalizing the differences using MAD.

        Parameters:
        ts1, ts2 (2D numpy arrays): Time series of shape (T, D)

        Returns:
        float: Normalized Manhattan distance.
        """
        if ts1.shape != ts2.shape:
            raise ValueError("Time series must have the same shape (T, D).")

        stack = np.vstack((ts1, ts2))
        mad_values = self.__mad_normalization(stack)  # Compute MAD over both series
        return np.sum(np.abs(ts1 - ts2) / mad_values)
