import pandas as pd
import numpy as np
from confetti.explainer.utils import load_data, convert_string_to_array
import keras
import config as cfg


class Evaluator:
    """
    Evaluates the quality of counterfactual explanations generated by an explainer.
    Computes key metrics such as sparsity, confidence, validity, and proximity.
    """

    def __init__(self, explainer: str):
        """
        Initializes the Evaluator with a specific explainer name.

        Args:
            explainer (str): The name of the explainer whose counterfactuals will be evaluated.
        """
        self.explainer = explainer

    def evaluate_dataset(self, dataset: str):
        """
        Evaluates the counterfactual explanations for a given dataset.

        Args:
            dataset (str): The name of the dataset to evaluate.

        Returns:
            tuple: A tuple containing:
                - DataFrame with individual counterfactual metrics.
                - DataFrame summarizing the overall evaluation results.
        """
        # Load Data
        X_train, X_test, y_train, y_test = load_data(dataset, encode_labels=True)
        timesteps = X_train.shape[1]  # Number of time steps per instance
        channels = X_train.shape[2]  # Number of channels per instance

        # Load Model
        model = self.__get_model(dataset)

        # Get Counterfactuals
        counterfactuals = self.__get_counterfactuals(dataset)

        # Compute Metrics
        counterfactuals_metrics = self.__get_counterfactual_metrics(counterfactuals, model, X_test, timesteps, channels)
        dataset_summary = self.__get_dataset_summary(dataset, counterfactuals_metrics)

        return counterfactuals_metrics, dataset_summary

    def __get_model(self, dataset: str):
        """
        Loads the trained model associated with the dataset.

        Args:
            dataset (str): The dataset name.

        Returns:
            keras.Model: The trained Keras model.
        """
        return keras.models.load_model(str(cfg.TRAINED_MODELS_DIR / dataset / f'{dataset}_fcn.keras'))

    def __get_counterfactuals(self, dataset: str):
        """
        Loads the counterfactual solutions generated by the explainer.

        Args:
            dataset (str): The dataset name.

        Returns:
            DataFrame: A DataFrame containing the counterfactuals.
        """
        return pd.read_csv(cfg.RESULTS_DIR / dataset / f'{self.explainer}_counterfactuals.csv')

    def __get_counterfactual_metrics(self, counterfactuals, model, X_test, timesteps, channels):
        """
        Computes key evaluation metrics for each counterfactual.

        Args:
            counterfactuals (DataFrame): The counterfactuals to evaluate.
            model (keras.Model): The trained model for making predictions.
            X_test (numpy.ndarray): The test set containing original instances.
            timesteps (int): Number of time steps per instance.
            channels (int): Number of channels per instance.

        Returns:
            DataFrame: A DataFrame containing sparsity, confidence, validity, and proximity metrics.
        """
        metrics = pd.DataFrame(columns=['Sparsity', 'Confidence', 'Validity', 'Proximity'])

        for instance in range(len(counterfactuals)):
            original_label = np.argmax(model.predict(X_test[instance].reshape(1, timesteps, channels)))
            counterfactual = convert_string_to_array(counterfactuals.iloc[instance]['Solution'],
                                                     timesteps=timesteps, channels=channels)

            # Compute Sparsity (how many unchanged features)
            sparsity = np.mean(X_test[instance].flatten() == counterfactual.flatten())

            # Compute Confidence (how different the prediction probability is from the original class)
            confidence = 1 - model.predict(counterfactual.reshape(1, timesteps, channels))[0][original_label]

            # Compute Validity (whether the counterfactual leads to a class change)
            ce_label = np.argmax(model.predict(counterfactual.reshape(1, timesteps, channels)))
            valid = 0 if original_label == ce_label else 1

            # Compute Proximity (L1 distance to original instance)
            proximity = self.__l1_distance_mad(X_test[instance], counterfactual)

            # Store results
            row_results_dict = {'Sparsity': sparsity, 'Confidence': confidence, 'Validity': valid,
                                'Proximity': proximity}
            row_results_df = pd.DataFrame([row_results_dict])
            metrics = pd.concat([metrics, row_results_df], ignore_index=True)

        return metrics

    def __get_dataset_summary(self, dataset: str, metrics: pd.DataFrame):
        """
        Summarizes the evaluation metrics for a dataset by computing the mean values.

        Args:
            dataset (str): The dataset name.
            metrics (DataFrame): The computed metrics for each counterfactual.

        Returns:
            DataFrame: A DataFrame summarizing the evaluation results.
        """
        summary = {
            'Dataset': dataset,
            'Sparsity': metrics['Sparsity'].mean(),
            'Confidence': metrics['Confidence'].mean(),
            'Validity': metrics['Validity'].mean(),
            'Proximity': metrics['Proximity'].mean()
        }
        return pd.DataFrame([summary])

    def __mad_normalization(self, ts):
        """
        Compute the Median Absolute Deviation (MAD) for each feature in a multivariate time series.

        Parameters:
        ts (2D numpy array): Time series of shape (T, D)

        Returns:
        1D numpy array: MAD values for each feature (D,)
        """
        median = np.median(ts, axis=0)
        mad = np.median(np.abs(ts - median), axis=0)
        mad[mad == 0] = 1e-9  # Avoid division by zero
        return mad

    def __l1_distance_mad(self, ts1:np.array, ts2:np.array):
        """
        Compute the Manhattan (L1) distance between two multivariate time series,
        normalizing the differences using MAD.

        Parameters:
        ts1, ts2 (2D numpy arrays): Time series of shape (T, D)

        Returns:
        float: Normalized Manhattan distance.
        """
        if ts1.shape != ts2.shape:
            raise ValueError("Time series must have the same shape (T, D).")

        stack = np.vstack((ts1, ts2))
        mad_values = self.__mad_normalization(stack)  # Compute MAD over both series
        return np.sum(np.abs(ts1 - ts2) / mad_values)
